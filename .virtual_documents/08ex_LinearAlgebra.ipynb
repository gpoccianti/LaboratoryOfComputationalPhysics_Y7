import numpy as np
import pandas as pd
from scipy import linalg as la

import matplotlib.pyplot as plt
%matplotlib inline





#generate the dataset
N=1000
x1=np.random.normal(loc=0, scale= 1, size=N)
x2=x1+np.random.normal(loc=0, scale= 3, size=N)
x3=2*x1+x2
df=pd.DataFrame({"x1": x1, "x2": x2, "x3": x3})

#find the eigenvectors and the eigenvalues of the covariance matrix of the dataset
Cov=np.cov(df.T) #Covariance matrix: we want it to be 3x3
l, V= la.eig(Cov) 
eig=np.sort(np.real_if_close(l)) #Sorting and chosing real eigenvalues
print("Eigevalues (sorted): \n",eig)
print("eigenvectors: \n", V)


#Find the eigenvectors and eigenvalues using SVD
U, spectrum, Vt = np.linalg.svd(Cov)

l_svd = spectrum
V_svd = U
print("\n")
print("With SVD:")
eig_svd=np.sort(np.real_if_close(l_svd))
print("Eigenvalues (sorted): \n",eig_svd) #Sorting and chosing real eigenvalues
print("Eigenvectors: \n",V_svd)
print("\n")
print("Are the eigenvalues matching ?", np.allclose(eig,eig_svd))


#What percent of the total dataset's variability is explained by the principal components? 
#Given how the dataset was constructed, do these make sense? 
#Reduce the dimensionality of the system so that at least 99% of the total variability is retained.
print("\nPercent of the total datast's variability explained by PCA:")
Lambda=np.diag(l)
print (np.real_if_close(Lambda[0,0]/Lambda.trace()))


#Redefine the data in the basis yielded by the PCA procedure
#eig=V[0,:] #eigenvalue corresponding to the PCA
#df_newc=pd.DataFrame(np.real_if_close(np.dot(V.T,df.T))) #We progect on PC
#df_newc

X=df.T
# rotate all the data points accordingly to the new base
Xp = pd.DataFrame(np.dot(V.T, X))
Xp.T





def scatter_PCA(ax1,ax2,X,ax):
    """
    X: our dataframe
    ax1,ax2: axis between which we want to see the projected PCA
    """
    scale_factor=3
    Cov=np.cov(X.T)
    l0, V0= la.eig(Cov) 
    l0=np.real_if_close(l0)
#    plt.figure(figsize=(10,10))
    ax.scatter(X.iloc[:,ax1], X.iloc[:,ax2], alpha=0.2)
    for li, vi in zip(l0, V0.T):
        plt.plot([0, scale_factor*li*vi[0]], [0, scale_factor*li*vi[1]], 'r-', lw=2)
        plt.axis([-3,3,-3,3]);


fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 7))

# Flatten the 2D array of axes to individual variables
ax1, ax2, ax3, ax4, ax5, ax6 = axes.flatten()
#fig, ax1 = plt.subplots(figsize=(10, 10))
"""
    X: our dataframe
    ax1,ax2: axis between which we want to see the projected PCA
"""

#Original coordinates
ii = 0
X = df
for cmp1, cmp2 in zip([0, 0, 1], [1, 2, 2]): #Component to visualize the plot
    ax1 = axes.flatten()[ii]
    ii += 1
    scale_factor = 3
    Cov = np.cov(X.T)
    l0, V0 = la.eig(Cov) 
    l0 = np.real_if_close(l0)
    
    # Scatter plot
    ax1.scatter(X.iloc[:, cmp1], X.iloc[:, cmp2], alpha=0.2)
    
    # Plot eigenvectors
    for li, vi in zip(l0, V0.T):
        ax1.plot([0, scale_factor * li * vi[cmp1]], 
                 [0, scale_factor * li * vi[cmp2]], 
                 'r-', lw=2)
    ax1.axis([-3, 3, -3, 3])

#New coordinates
X=Xp.T
for cmp1, cmp2 in zip([0, 0, 1], [1, 2, 2]): #Component to visualize the plot
    ax1 = axes.flatten()[ii]
    ii += 1
    scale_factor = 3
    Cov = np.cov(X.T)
    l0, V0 = la.eig(Cov) 
    l0 = np.real_if_close(l0)
    
    # Scatter plot
    ax1.scatter(X.iloc[:, cmp1], X.iloc[:, cmp2], alpha=0.2)
    
    # Plot eigenvectors
    for li, vi in zip(l0, V0.T):
        ax1.plot([0, scale_factor * li * vi[cmp1]], 
                 [0, scale_factor * li * vi[cmp2]], 
                 'r-', lw=2)
    ax1.axis([-3, 3, -3, 3])






noise = [] #list of noisy channels (dimenions)

for i in np.arange(5):
    noise.append(np.random.normal(loc=0, scale= 1/50, size=N))
    df["noise"+str(i)]= noise[i]

df.head()


fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(12, 7))

# Flatten the 2D array of axes to individual variables
ax1, ax2, ax3, ax4, ax5, ax6 = axes.flatten()
#fig, ax1 = plt.subplots(figsize=(10, 10))
"""
    X: our dataframe
    ax1,ax2: axis between which we want to see the projected PCA
"""

#Original coordinates
ii = 0
X = df
for cmp1, cmp2 in zip([0, 0, 1], [1, 2, 2]): #Component to visualize the plot
    ax1 = axes.flatten()[ii]
    ii += 1
    scale_factor = 3
    Cov = np.cov(X.T)
    l0, V0 = la.eig(Cov) 
    l0 = np.real_if_close(l0)
    
    # Scatter plot
    ax1.scatter(X.iloc[:, cmp1], X.iloc[:, cmp2], alpha=0.2)
    
    # Plot eigenvectors
    for li, vi in zip(l0, V0.T):
        ax1.plot([0, scale_factor * li * vi[cmp1]], 
                 [0, scale_factor * li * vi[cmp2]], 
                 'r-', lw=2)
    ax1.axis([-3, 3, -3, 3])

#New coordinates
X=Xp.T
for cmp1, cmp2 in zip([0, 0, 1], [1, 2, 2]): #Component to visualize the plot
    ax1 = axes.flatten()[ii]
    ii += 1
    scale_factor = 3
    Cov = np.cov(X.T)
    l0, V0 = la.eig(Cov) 
    l0 = np.real_if_close(l0)
    
    # Scatter plot
    ax1.scatter(X.iloc[:, cmp1], X.iloc[:, cmp2], alpha=0.2)
    
    # Plot eigenvectors
    for li, vi in zip(l0, V0.T):
        ax1.plot([0, scale_factor * li * vi[cmp1]], 
                 [0, scale_factor * li * vi[cmp2]], 
                 'r-', lw=2)
    ax1.axis([-3, 3, -3, 3])









# get the dataset and its description on the proper data directory
import os
#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data -P ~/data/
#!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.names -P ~/data/ 

#create the directory
# Step 1: Create the 'data' folder if it doesn't exist
os.makedirs('data', exist_ok=True)

#Step 2: Install locally instead than home directory:
import os

# Step 1: Create the 'data' folder if it doesn't exist
os.makedirs('data', exist_ok=True)

# Step 2: Use wget to download the file into the 'data' folder
!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.data -O ./data/magic04.data
!wget https://archive.ics.uci.edu/ml/machine-learning-databases/magic/magic04.names -O ./data/magic04.names
